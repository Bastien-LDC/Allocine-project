{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Data From AlloCiné.fr\n",
    "\n",
    "This script builds a DataFrame by web scraping the data from AlloCiné — a company which provides information on French cinema. Because of the long delay, we choose to scrape the data in two steps : \n",
    "- First we scrape the url of each movie with `getMoviesUrl()`\n",
    "- Lastly we use the url list to scrape the data for each movie with `ScrapeURL()`\n",
    "\n",
    "*Note : We use the popular BeautifulSoup package*\n",
    "\n",
    "## Functions :\n",
    "\n",
    "### `getMoviesUrl(start_page, end_page)` :\n",
    "\n",
    "Save a CSV files of the url list as `movie_url.csv`. The argument must be integers and are used to select the range of page you want to scrape the data from. The `end_page` is not include.\n",
    "\n",
    "### `ScrapeURL(movie_url)` :\n",
    "\n",
    "Iterate over the list of url generate by `getMoviesUrl()` and scrape the data for each movie. In the process, we extract :\n",
    "\n",
    "- `movie_title` : the movies title (in french)\n",
    "- `release_date`: the original release date\n",
    "- `re_release_date`: the re-release date\n",
    "- `duration`: the movies length\n",
    "- `genre` : the movies types (as an array, up to three different types)\n",
    "- `directors` : movies directors (as an array)\n",
    "- `actors` : main characters of the movies (as an array)\n",
    "- `nationality`: nationality of the movies (as an array)\n",
    "- `press_rating`: press ratings (from 0 to 5 stars)\n",
    "- `nber_press_vote`: number of press votes\n",
    "- `user_rating`:  AlloCiné users ratings (from 0 to 5 stars)\n",
    "- `nber_user_vote`: number of users votes\n",
    "\n",
    "The function `ScrapeURL()` returns two objects : the data as a dataframe and the url list of error as a list. In addition the two objects are saved as `allocine_movies.csv` and `allocine_errors.csv`. You could pass the list of errors into `ScrapeURL()` to get the extra data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from requests import get\n",
    "from time import time\n",
    "from time import sleep\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import dateparser\n",
    "\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to scrape the movies urls from http://www.allocine.fr/films/\n",
    "# Choose the page range with the two parameters start_page and end_page.\n",
    "# The url list is save as a csv file: movie_url.csv\n",
    "def getMoviesUrl(start_page, end_page):\n",
    "    # Set the list\n",
    "    movie_url = []\n",
    "    \n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = start_page\n",
    "    m_requests = 0\n",
    "        \n",
    "    for p in range(start_page, end_page):\n",
    "\n",
    "        # Get request\n",
    "        url = 'http://www.allocine.fr/films/?page={}'.format(str(p))\n",
    "        response = get(url)\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(randint(1,2))\n",
    "            \n",
    "        # Monitoring the requests\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Page Request: {}; Frequency: {} requests/s'.format(p_requests, p_requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "            \n",
    "        # Warning for non-200 status codes\n",
    "        if response.status_code != 200:\n",
    "            warn('Page Request: {}; Status code: {}'.format(p_requests, response.status_code))\n",
    "\n",
    "        # Break the loop if the number of requests is greater than expected\n",
    "        if p_requests > end_page:\n",
    "            warn('Number of requests was greater than expected.')\n",
    "            break\n",
    "\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Select all the movies url from a single page\n",
    "        movies = html_soup.find_all('h2', 'meta-title')\n",
    "        m_requests += len(movies)\n",
    "        \n",
    "        # Monitoring the requests\n",
    "        print('Page Request: {}; Movie Request: {}'.format(p_requests, m_requests))\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        # Pause the loop\n",
    "        sleep(1)\n",
    "        \n",
    "        for movie in movies:\n",
    "            movie_url.append('http://www.allocine.fr{}'.format(movie.a['href']))\n",
    "        \n",
    "        p_requests += 1\n",
    "    \n",
    "    # Saving the files\n",
    "    r = np.asarray(movie_url)\n",
    "    np.savetxt(\"movie_url.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Request: 3999; Movie Request: 59985\n"
     ]
    }
   ],
   "source": [
    "# We use it to scrape the first 3999 pages\n",
    "getMoviesUrl(1, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to scrape the data from the movies urls\n",
    "# The function return a dataframe and a list of url that return error.\n",
    "# And save them into csv files (allocine_movies.csv and allocine_errors.csv)\n",
    "def ScrapeURL(movie_url):\n",
    "    \n",
    "    # init the dataframe\n",
    "    c = ['title', 'date_reprise', 'date_sortie', 'duration', 'director', 'actor', 'genre', 'nationality', \n",
    "     'press_rating', 'nb_press', 'spec_rating', 'nb_spec']\n",
    "    df = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    n_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors = []\n",
    "    \n",
    "    # request loop\n",
    "    for url in movie_url:\n",
    "        try :\n",
    "            response = get(url)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Monitoring the requests\n",
    "            n_request += 1\n",
    "            \n",
    "            elapsed_time = time() - start_time\n",
    "            print('Request #{}; Frequency: {} requests/s'.format(n_request, n_request/elapsed_time))\n",
    "            clear_output(wait = True)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Warning for non-200 status codes\n",
    "            if response.status_code != 200:\n",
    "                warn('Request #{}; Status code: {}'.format(n_request, response.status_code))\n",
    "                errors.append(url)\n",
    "\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            movie_html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            if movie_html_soup.find('div', 'titlebar-title'):\n",
    "                # Scrape the title\n",
    "                tp_title = movie_html_soup.find('div', 'titlebar-title').text\n",
    "\n",
    "                # Set cursors\n",
    "                the_movie = movie_html_soup.section.div.div.div\n",
    "                movie_info = the_movie.select('.meta-body-item')\n",
    "                rating_info = the_movie.select('.rating-item')\n",
    "\n",
    "                # Scrape extra info\n",
    "\n",
    "                # Set the defaut value\n",
    "                tp_dt_reprise = np.nan\n",
    "                tp_dt_sortie = np.nan\n",
    "                tp_duration = np.nan\n",
    "                tp_nation = np.nan\n",
    "                tp_director = []\n",
    "                tp_actor = []\n",
    "                tp_genre = []\n",
    "\n",
    "                for i in movie_info:\n",
    "                    if (i.span):\n",
    "                        # Scrape the dates\n",
    "                        if i.span.text == 'Date de reprise':\n",
    "                            tp_dt_reprise = dateparser.parse(i.strong.span.text)\n",
    "                        elif i.span.text == 'Date de sortie':\n",
    "                            i.span.decompose()\n",
    "                            if i.span:\n",
    "                                tp_dt_sortie = dateparser.parse(i.span.text)\n",
    "                                i.span.decompose()\n",
    "                            tp_duration = (i.text).strip().replace('(', '').replace(')', '')\n",
    "                        # Scrape the directors\n",
    "                        elif i.span.text == 'De':\n",
    "                            tp_director = [t.text for t in i.select(\"a span\")]\n",
    "                        # Scrape the actors\n",
    "                        elif i.span.text == 'Avec':\n",
    "                            if i.find('span', 'more'):\n",
    "                                i.find('span', 'more').decompose()\n",
    "                            tp_actor = [t.text for t in i.select(\".blue-link\")]\n",
    "                        # Scrape the genres\n",
    "                        elif (i.span.text == 'Genre') | (i.span.text == 'Genres'):\n",
    "                            tp_genre = [t.text for t in i.select(\".blue-link\")]\n",
    "                        # Scrape the nationnality\n",
    "                        elif (i.span.text == 'Nationalité') | (i.span.text == 'Nationalités'):\n",
    "                            tp_nation = [t.text.strip() for t in i.select('.nationality')]\n",
    "\n",
    "                # Scrape the ratings\n",
    "\n",
    "                # Set the defaut value\n",
    "                tp_press_rating = np.nan\n",
    "                tp_nb_press_rating = np.nan\n",
    "                tp_spec_rating = np.nan\n",
    "                tp_nb_spec_rating = np.nan\n",
    "\n",
    "                for r in rating_info:\n",
    "                    if (r.span):\n",
    "                        # Scrape the press ratings\n",
    "                        if r.span.text.strip() == 'Presse':\n",
    "                            tp_press_rating = r.div.span.text.strip().replace(',','.')\n",
    "                            tp_nb_press_rating = r.div.span.next_sibling.text.strip().split(' ')[0]\n",
    "                        # Scrape the users ratings\n",
    "                        elif r.span.text.strip() == 'Spectateurs':\n",
    "                            tp_spec_rating = r.div.span.text.strip().replace(',','.')\n",
    "                            tp_nb_spec_rating = r.div.span.next_sibling.text.strip().split(' ')[0]\n",
    "\n",
    "                # Append the data\n",
    "                df_tmp = pd.DataFrame({'title': [tp_title],\n",
    "                                       'date_reprise': [tp_dt_reprise],\n",
    "                                       'date_sortie': [tp_dt_sortie],\n",
    "                                       'duration': [tp_duration],\n",
    "                                       'director': [tp_director],\n",
    "                                       'actor': [tp_actor],\n",
    "                                       'genre': [tp_genre],\n",
    "                                       'nationality': [tp_nation],\n",
    "                                       'press_rating': [tp_press_rating],\n",
    "                                       'nb_press': [tp_nb_press_rating],\n",
    "                                       'spec_rating': [tp_spec_rating],\n",
    "                                       'nb_spec': [tp_nb_spec_rating]})\n",
    "                \n",
    "                df = pd.concat([df, df_tmp], ignore_index=True)\n",
    "        except:\n",
    "            errors.append(url)\n",
    "            warn('Request #{} fail; Total errors : {}'.format(n_request, len(errors)))\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    elapsed_time = time() - start_time\n",
    "    print('Done; {} requests in {} seconds with {} errors'.format(n_request, round(elapsed_time, 0), len(errors)))\n",
    "    clear_output(wait = True)\n",
    "    df.to_csv(\"allocine_movies.csv\")\n",
    "    errors.to_csv(\"allocine_errors.csv\")\n",
    "    # return dataframe and errors\n",
    "    return df, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the list of urls \n",
    "m_url = pd.read_csv(\"movie_url.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrape the data \n",
    "d, e = ScrapeURL(m_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
