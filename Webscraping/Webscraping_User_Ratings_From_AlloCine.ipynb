{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping User Ratings From AlloCiné.fr\n",
    "\n",
    "This script builds a DataFrame by web scraping the data from AlloCiné — this time, its purpose is to retrieve the user and press ratings of movies and series from the website of AlloCiné. We will proceed as follows: \n",
    "- We use the series and movies url lists generated in the other scripts to get the comments section urls of the press and the spectators (users) with `getCommentsUrl()`.\n",
    "- We use the press comments urls for movies and series to get the press ratings with `getPressRatings()`.\n",
    "- We use the user comments urls for movies and series to get the user ratings with `getUserRatings()`.\n",
    "- From there, we scrape the user ID and their rating for each movie or series with `ScrapeURL()`.\n",
    "- The user can be either a person or a press newspaper. Separated files are generated for each type of user.\n",
    "\n",
    "*Note : We use the popular BeautifulSoup package*\n",
    "\n",
    "## Functions :\n",
    "\n",
    "### `getCommentsUrl(movies_df, series_df, spect, press)`\n",
    "\n",
    "This function will call two sub-functions: `getMoviesCommentsUrl(movies_df, spect, press)` and `getSeriesCommentsUrl(series_df, spect, press)`. Both respectively iterate over the list of movies and series url generated by `getMoviesUrl()` in the previous scripts and get the comments section url. We can chose to get the comments section from the user or the press for each video type.\n",
    "We then store the lists of urls in a csv file entitled `user_comments_url.csv` (resp. `press_comments_url.csv`), in both movies and series directory (`../Movies/Comments/` and `../Series/Comments/`).\n",
    "\n",
    "### `ScrapeURL(urls)` :\n",
    "\n",
    "Iterate over the list of movies or series comments section url generated by `getCommentsUrl()` and scrape the data for each movie or series ratings. In the process, we extract :\n",
    "\n",
    "- `user_id` : Allocine user id (person or press)\n",
    "- `id` : Allocine movie or series id\n",
    "- `user_rating`: AlloCiné users ratings (from 0.5 to 5 stars) \n",
    "\n",
    "\n",
    "The function `ScrapeURL()` returns two objects : the user_rating and press-rating as two dataframes. In addition, the two objects are saved as `user_ratings_movies.csv` and `press_ratings_movies.csv` (respectively `user_ratings_series.csv` and `press_ratings_series.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------\n",
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_2088\\95561895.py:13: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from time import sleep\n",
    "from datetime import timedelta\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SUPPRIMER APRES TESTS\n",
    "response = urlopen(\"https://www.allocine.fr/film/fichefilm-260627/critiques/spectateurs/membres-critiques/\")\n",
    "html_text = response.read().decode(\"utf-8\")\n",
    "press_html_soup = BeautifulSoup(html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `getMoviesCommentsUrl(movies_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoviesCommentsUrl(movies_df: pd.DataFrame, spect=False, press=False):\n",
    "    '''\n",
    "    Get the comments section url for each movie\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in a csv file\n",
    "    '''    \n",
    "    # Get the list of movies_id from the movies_df\n",
    "    movies_id_list = movies_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in movies_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descencing number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/spectateurs/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Movies/Comments/'\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True) #create folders if not exists\n",
    "    print(f'--> Done; {p_requests-1} Movies Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `getSeriesCommentsUrl(series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeriesCommentsUrl(series_df: pd.DataFrame, spect=False, press=False):\n",
    "    '''\n",
    "    Get the comments section url for each series\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in a csv file\n",
    "    '''\n",
    "    # Get the list of series_id from the series_df\n",
    "    series_id_list = series_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in series_id_list:\n",
    "        if spect:\n",
    "            # Url for the spectators/user section sorted by the descencing number of reviews per user\n",
    "            url_spect = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/membres-critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Series/Comments/'\n",
    "    os.makedirs(os.path.dirname(comments_path), exist_ok=True) #create folders if not exists\n",
    "    print(f'--> Done; {p_requests-1} Series Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommentsUrl(movies_df=None, series_df=None, spect=False, press=False):\n",
    "    '''\n",
    "    Get the comments section url for each movie\n",
    "    You can select the spectateurs or presse section, or both.\n",
    "    :param movies_df: DataFrame of movies\n",
    "    :param series_df: DataFrame of series\n",
    "    :param spect: Boolean, True if you want to get the spectateurs section\n",
    "    :param press: Boolean, True if you want to get the presse section\n",
    "    :return: nothing but saves urls in a csv file\n",
    "    '''\n",
    "    try:\n",
    "        if movies_df is not None:\n",
    "            getMoviesCommentsUrl(movies_df, spect, press)\n",
    "        if series_df is not None:\n",
    "            getSeriesCommentsUrl(series_df, spect, press)\n",
    "    except:\n",
    "        print('Error in getCommentsUrl function!')\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get press-ratings dataframe: `getPressRatings(urls)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to rating: `convertTextToRating(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_rating(text):\n",
    "    '''\n",
    "    Convert text to rating\n",
    "    :param text: Evaluation of the movie as a string \n",
    "    :return: corresponding float rating value\n",
    "    '''\n",
    "    if text==\"Nul\":\n",
    "        return 0.5\n",
    "    elif text==\"Très mauvais\":\n",
    "        return 1\n",
    "    elif text==\"Mauvais\":\n",
    "        return 1.5\n",
    "    elif text==\"Pas terrible\":\n",
    "        return 2\n",
    "    elif text==\"Moyen\":\n",
    "        return 2.5  \n",
    "    elif text==\"Pas mal\":\n",
    "        return 3\n",
    "    elif text==\"Bien\":\n",
    "        return 3.5\n",
    "    elif text==\"Très bien\":\n",
    "        return 4\n",
    "    elif text==\"Excellent\":\n",
    "        return 4.5\n",
    "    elif text==\"Chef d'oeuvre\":\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `getPressRatings(urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPressRatings(press_soup):\n",
    "    '''\n",
    "    Get press_ratings from the comments section url for each movie or series.\n",
    "    :param press_soup: BeautifulSoup object of the press comments section url\n",
    "    :return: a dictionary of ratings (key: user_id, value: press rating) if the ratings are available, else None.\n",
    "    '''\n",
    "    press_ratings = {}\n",
    "    div_ratings = press_soup.find_all('li', {'class': 'item'})\n",
    "    if div_ratings:\n",
    "        press_ratings = {id.text.strip():convert_text_to_rating(rating.find('span')['title']) for id, rating in zip(div_ratings, div_ratings)}\n",
    "        return press_ratings\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get user-ratings dataframe: `getUserRatings(urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserRatings(user_soup, nb_users: int, min_nb_reviews: int):\n",
    "    '''\n",
    "    Get user_ratings from the comments section url for each movie or series.\n",
    "    We will keep only the nb_users first users who have posted at least min_nb_reviews number of reviews.\n",
    "    :param user_soup: BeautifulSoup object of the user comments section url.\n",
    "    :param nb_users: Number of users to keep.\n",
    "    :param min_nb_reviews: Minimum number of reviews per user.\n",
    "    :return: a dictionary of ratings (key: user_id, value: user rating) if the ratings are available, else None.\n",
    "    '''\n",
    "    user_ratings = {}\n",
    "    div_ratings = user_soup.find_all('div', {'class': 'review-card'})\n",
    "    if div_ratings:\n",
    "        user_ratings = {id.find_all('span')[3]['data-targetuserid']:rating for id, rating in zip(div_ratings, div_ratings)}\n",
    "        return user_ratings\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"hred review-card cf\" id=\"review_1019631083\">\n",
       "<div class=\"review-card-aside\">\n",
       "<div class=\"review-card-user-infos cf\">\n",
       "<figure class=\"thumbnail\">\n",
       "<span class=\"ACrL21ACrlbWJyZS1aMjAwMzA5MTYxMTUzMTA5NTM3MDEzMjQv thumbnail-container thumbnail-link\" title=\"selenie\">\n",
       "<img alt=\"selenie\" class=\"thumbnail-img\" data-src=\"https://fr.web.img6.acsta.net/c_80_80/i_w_h/avatar/FR/4/2/3/Z20030916115310953701324/img/ytsrhhcf.ckm.jpg\" height=\"80\" src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAAAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==\" width=\"80\"/>\n",
       "</span>\n",
       "</figure>\n",
       "<div class=\"meta\">\n",
       "<div class=\"meta-title\">\n",
       "<span class=\"ACrL21ACrlbWJyZS1aMjAwMzA5MTYxMTUzMTA5NTM3MDEzMjQv\">selenie</span>\n",
       "</div>\n",
       "<span class=\"ACrL2NACrsdWIzMDAv\"><img class=\"member-club300\" data-src=\"https://assets.allocine.fr/skin/img/member-club300-7274b99a00.svg\" height=\"20\" src=\"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAAAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw==\" title=\"Membre du Club 300 Allociné\" width=\"80\"/></span>\n",
       "<p class=\"meta-content light\">\n",
       "<span class=\"item-profil js-follow-unfollow\" data-targetuserid=\"Z20030916115310953701324\" data-totalreviews=\"5405\">\n",
       "Suivre son activité\n",
       "</span>\n",
       "<span class=\"ACrL21ACrlbWJyZS1aMjAwMzA5MTYxMTUzMTA5NTM3MDEzMjQvY29tbXVuYXV0ZS9hYm9ubmVzLw== item-profil\">\n",
       "3 049 abonnés\n",
       "</span>\n",
       "<span class=\"ACrL21ACrlbWJyZS1aMjAwMzA5MTYxMTUzMTA5NTM3MDEzMjQvY3JpdGlxdWVzL2ZpbG1zLw== item-profil\">\n",
       "Lire ses 5 405 critiques\n",
       "</span>\n",
       "</p>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"review-card-review-holder\">\n",
       "<div class=\"review-card-meta\">\n",
       "<div class=\"stareval stareval-medium stareval-theme-default\"><div class=\"rating-mdl n20 stareval-stars\"><div class=\"star icon\"></div><div class=\"star icon\"></div><div class=\"star icon\"></div><div class=\"star icon\"></div><div class=\"star icon\"></div></div><span class=\"stareval-note\">2,0</span></div>\n",
       "<span class=\"review-card-meta-date light\">\n",
       "Publiée le 31 mars 2022\n",
       "</span>\n",
       "</div>\n",
       "\n",
       "<div class=\"content-txt review-card-content\">\n",
       "Le film débute de façon si banale qu'on est déjà peu enclin à espérer un grand film. Le pire dans ce film est un scénario d'une banalité affligeante qui reprend un canevas qu'on a déjà vu mille fois, qui emprunte à plusieurs autres super-héros sans trouver la moindre idée qui ferait le truc en plus. Pourtant, sa soif de sang alors qu'il est un chercheur humain plein d'abnégation est un paramètre passionnant mais qui n'est jamais transcender car rappelons que Morbius est plus un anti-héros qu'un réel super-méchant ; c'est sa soif de sang qui le transforme mais il reste foncièrement bon. Ainsi les scènes post-génériques finissent d'abattre la cohérence qui pouvait lier les deux facettes du personnages. Niveau acting le pire reste la belle de la bête poupée du nom de Martine jouée par un ersatz de Jessica Alba qui est certe très belle mais un peu fade sans étincelle. Les scènes d'action sont également une déception, trop fouillies, peu lisibles, avec un abus constant et omniprésent d'une sorte de bullet time fumeuse. Oubliable.<br/>Site : Selenie\n",
       "</div>\n",
       "<div class=\"review-card-social\">\n",
       "<div class=\"reviews-users-comment-useful js-useful-reviews\" data-opinionid=\"1019631083\" data-statistics='{\"helpfulCount\":1,\"unhelpfulCount\":1}' data-totalreviews=\"216\">\n",
       "<a class=\"button button-xs button-helpful button-disabled\" href=\"#\">\n",
       "<i class=\"icon icon-left icon-smiley-happy\"></i><span class=\"txt\">1</span>\n",
       "</a>\n",
       "<a class=\"button button-xs button-helpful button-disabled\" href=\"#\">\n",
       "<i class=\"icon icon-left icon-smiley-sad\"></i><span class=\"txt\">1</span>\n",
       "</a>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "press_html_soup.find_all('div', {'class': 'review-card'})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `ScrapeURL(urls)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `ScrapePressURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the user_ratings from the press and user comments section urls\n",
    "# Returns a datframe and save it as 'movies_press_ratings.csv' (or 'series_press_ratings.csv')\n",
    "def ScrapePressURL(series_url: list=None, movies_url: list=None):        \n",
    "    # init the dataframes\n",
    "    c = [\"user_id\",\n",
    "        \"id\",\n",
    "        \"user_rating\",\n",
    "    ]\n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return None\n",
    "    if series_url is not None:\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        for url in series_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Series Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id\n",
    "                series_id = url.split('/')[-4].split('-')[-1]            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'user_id': [id],\n",
    "                                        'id': [series_id],\n",
    "                                        'user_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series Request #{ns_request} fail; Press rating does not exist! Total errors : {len(errors_series)}')\n",
    "                traceback.print_exc()\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        for url in movies_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Movie Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id\n",
    "                movie_id = url.split('/')[-4].split('-')[-1]            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'user_id': [id],\n",
    "                                        'id': [movie_id],\n",
    "                                        'user_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie Request #{nm_request} fail; Press rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    if series_url is not None:\n",
    "        series_path = '../Series/Ratings/'\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {ns_request} Series Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no press ratings)')\n",
    "        df_series.to_csv(f\"{series_path}press_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}press_ratings_errors.csv\")\n",
    "    if movies_url is not None:\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {nm_request} Movies Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no press ratings)')\n",
    "        df_movies.to_csv(f\"{movies_path}press_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}press_ratings_errors.csv\")\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `ScrapeUserURL(series_urls, movies_urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScrapeUserURL(series_url: list, movies_url: list):\n",
    "    # init the dataframes\n",
    "    c = [\"user_id\",\n",
    "        \"id\",\n",
    "        \"user_rating\",\n",
    "    ]\n",
    "    if series_url is None and movies_url is None:\n",
    "        print('No url to scrape!')\n",
    "        return None\n",
    "    if series_url is not None:\n",
    "        df_series = pd.DataFrame(columns=c)\n",
    "    if movies_url is not None:\n",
    "        df_movies = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    ns_request = 0\n",
    "    nm_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors_series = []\n",
    "    errors_movies = []\n",
    "    \n",
    "    if series_url is not None:\n",
    "        # request loop\n",
    "        for url in series_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                ns_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Series Request #{ns_request}; Frequency: {ns_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Series Request #{}; Status code: {}'.format(ns_request, response.status_code))\n",
    "                    errors_series.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the series_id\n",
    "                series_id = url.split('/')[-3].split('-')[-1] \n",
    "                # Get the user_ratings\n",
    "                user_ratings = getUserRatings(press_html_soup)\n",
    "                \n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_series_tmp = pd.DataFrame({'user_id': [id],\n",
    "                                        'id': [series_id],\n",
    "                                        'user_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_series = pd.concat([df_series, df_series_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_series.append(url)\n",
    "                warn(f'Series Request #{ns_request} fail; Press rating does not exist! Total errors : {len(errors_series)}')\n",
    "                traceback.print_exc()\n",
    "    if movies_url is not None:\n",
    "        # request loop\n",
    "        for url in movies_url:\n",
    "            try :\n",
    "                response = urlopen(url)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Monitoring the requests\n",
    "                nm_request += 1\n",
    "                \n",
    "                elapsed_time = time() - start_time\n",
    "                print(f'Movie Request #{nm_request}; Frequency: {nm_request/elapsed_time} requests/s')\n",
    "                clear_output(wait = True)\n",
    "\n",
    "                # Pause the loop\n",
    "                sleep(randint(1,2))\n",
    "\n",
    "                # Warning for non-200 status codes\n",
    "                if response.status != 200:\n",
    "                    warn('Movie Request #{}; Status code: {}'.format(nm_request, response.status_code))\n",
    "                    errors_movies.append(url)\n",
    "\n",
    "                # Parse the content of the request with BeautifulSoup\n",
    "                html_text = response.read().decode(\"utf-8\")\n",
    "                press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "                # Get the movie_id\n",
    "                movie_id = url.split('/')[-4].split('-')[-1]            \n",
    "                # Get the press_ratings\n",
    "                press_ratings = getPressRatings(press_html_soup)\n",
    "                for id, rating in press_ratings.items():               \n",
    "                    # Append the data\n",
    "                    df_movies_tmp = pd.DataFrame({'user_id': [id],\n",
    "                                        'id': [movie_id],\n",
    "                                        'user_rating': [rating]\n",
    "                                        })                                    \n",
    "                    df_movies = pd.concat([df_movies, df_movies_tmp], ignore_index=True)                \n",
    "            except:\n",
    "                errors_movies.append(url)\n",
    "                warn(f'Movie Request #{nm_request} fail; Press rating does not exist! Total errors : {len(errors_movies)}')\n",
    "                traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    if series_url is not None:\n",
    "        series_path = '../Series/Ratings/'\n",
    "        os.makedirs(os.path.dirname(series_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {ns_request} Series Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_series)} errors (series with no press ratings)')\n",
    "        df_series.to_csv(f\"{series_path}press_ratings_series.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_series = pd.DataFrame(errors_series, columns=['url'])\n",
    "        errors_df_series.to_csv(f\"{series_path}press_ratings_errors.csv\")\n",
    "    if movies_url is not None:\n",
    "        movies_path = '../Movies/Ratings/'\n",
    "        os.makedirs(os.path.dirname(movies_path), exist_ok=True) #create folders if not exists\n",
    "        elapsed_time = time() - start_time\n",
    "        print(f'Done; {nm_request} Movies Press Ratings requests in {timedelta(seconds=elapsed_time)} with {len(errors_movies)} errors (movies with no press ratings)')\n",
    "        df_movies.to_csv(f\"{movies_path}press_ratings_movies.csv\", index=False)\n",
    "        # list to dataframe\n",
    "        errors_df_movies = pd.DataFrame(errors_movies, columns=['url'])\n",
    "        errors_df_movies.to_csv(f\"{movies_path}press_ratings_errors.csv\")\n",
    "    \n",
    "    # return dataframe and errors\n",
    "    return df_series, df_movies, errors_series, errors_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the movies and series dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movies and series dataframes\n",
    "def loadDataFrames():\n",
    "    '''\n",
    "    Load the movies and series dataframes\n",
    "    :return: movies and series dataframes\n",
    "    '''\n",
    "    movies_df = pd.read_csv('../Movies/Data/allocine_movies.csv')\n",
    "    series_df = pd.read_csv('../Series/Data/allocine_series.csv')\n",
    "    return movies_df, series_df\n",
    "movies_df, series_df = loadDataFrames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the comments section urls from spectators and press for movies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 298 Movies Comments Page Requests in 0:00:00.001084\n",
      "--> Done; 296 Series Comments Page Requests in 0:00:00\n"
     ]
    }
   ],
   "source": [
    "getCommentsUrl(movies_df=movies_df, series_df=series_df, spect=True, press=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the comments section urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the press_comments_urls for series\n",
    "press_series_url = pd.read_csv(\"../Series/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_series_url = press_series_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the user_comments_urls for series\n",
    "user_series_url = pd.read_csv(\"../Series/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_series_url = user_series_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the press_comments_urls for movies\n",
    "press_movies_url = pd.read_csv(\"../Movies/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_movies_url = press_movies_url['url'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the users_comments_urls for movies\n",
    "user_movies_url = pd.read_csv(\"../Movies/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_movies_url = user_movies_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done; 296 Series Press Ratings requests in 0:33:11.566262 with 134 errors\n",
      "Done; 288 Movies Press Ratings requests in 0:33:11.584518 with 70 errors\n"
     ]
    }
   ],
   "source": [
    "# Scraping the press ratings from series and/or movies\n",
    "press_series_ratings, press_movies_ratings, errors_series_ratings, errors_movies_ratings = ScrapePressURL(series_url=press_series_url, movies_url=press_movies_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variety                    75\n",
       "Télérama                   74\n",
       "The Hollywood Reporter     57\n",
       "Le Parisien                57\n",
       "Le Monde                   51\n",
       "                           ..\n",
       "Season One                  1\n",
       "Nice-Matin                  1\n",
       "Philadelphia Daily News     1\n",
       "Red Eye                     1\n",
       "The Salt Lake Tribune       1\n",
       "Name: user_id, Length: 120, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort df_press_ratings by user_id\n",
    "press_series_ratings = press_series_ratings.sort_values(by=['user_id'],ignore_index=True)\n",
    "press_series_ratings.user_id.value_counts()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1774c64d83fce3825259fa3771bbe70271854497325f5fa1e2c1b92279703"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
