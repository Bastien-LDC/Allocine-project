{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping User Ratings From AlloCiné.fr\n",
    "\n",
    "This script builds a DataFrame by web scraping the data from AlloCiné — this time, its purpose is to retrieve the user and press ratings of movies and series from the website of AlloCiné. We will proceed as follows: \n",
    "- We use the series and movies url lists generated in the other scripts to get the comments section urls with `getCommentsUrl()`.\n",
    "- From there, we scrape the user ID and their rating for each movie with `ScrapeURL()`.\n",
    "- The user can be either a person or a press newspaper. Separated files are generated for each type of user.\n",
    "\n",
    "*Note : We use the popular BeautifulSoup package*\n",
    "\n",
    "## Functions :\n",
    "\n",
    "### `getCommentsUrl(movies_df, series_df, spect, press)`\n",
    "\n",
    "This function will call two sub-functions: `getMoviesCommentsUrl(movies_df, spect, press)` and `getSeriesCommentsUrl(series_df, spect, press)`. Both respectively iterate over the list of movies and series url generated by `getMoviesUrl()` in the previous scripts and get the comments section url. We can chose to get the comments section from the user or the press for each video type.\n",
    "We then store the lists of urls in a csv file entitled `user_comments_url.csv` (resp. `press_comments_url.csv`), in both movies and series directory (`../Movies/Comments/` and `../Series/Comments/`).\n",
    "\n",
    "### `ScrapeURL(urls)` :\n",
    "\n",
    "Iterate over the list of movies or series comments section url generated by `getCommentsUrl()` and scrape the data for each movie or series ratings. In the process, we extract :\n",
    "\n",
    "- `user_id` : Allocine user id (person or press)\n",
    "- `id` : Allocine movie or series id\n",
    "- `user_rating`: AlloCiné users ratings (from 0.5 to 5 stars) \n",
    "\n",
    "\n",
    "The function `ScrapeURL()` returns two objects : the user_rating and press-rating as two dataframes. In addition, the two objects are saved as `user_ratings_movies.csv` and `press_ratings_movies.csv` (respectively `user_ratings_series.csv` and `press_ratings_series.csv`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\AppData\\Local\\Temp\\ipykernel_24668\\678141690.py:15: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    }
   ],
   "source": [
    "# Import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time\n",
    "from time import sleep\n",
    "from datetime import timedelta\n",
    "from urllib.request import urlopen\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from warnings import warn\n",
    "from IPython.core.display import clear_output\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A SUPPRIMER APRES TESTS\n",
    "response = urlopen(\"https://www.allocine.fr/film/fichefilm-260627/critiques/presse/\")\n",
    "html_text = response.read().decode(\"utf-8\")\n",
    "press_html_soup = BeautifulSoup(html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `getMoviesCommentsUrl(movies_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the comments section url for each movie\n",
    "# You can select the spectateurs or presse section, or both\n",
    "def getMoviesCommentsUrl(movies_df: pd.DataFrame, spect=False, press=False):\n",
    "    \n",
    "    # Get the list of movies_id from the movies_df\n",
    "    movies_id_list = movies_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in movies_id_list:\n",
    "        if spect:\n",
    "            url_spect = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/spectateurs/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/film/fichefilm-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Movies/Comments/'\n",
    "    print(f'--> Done; {p_requests-1} Movies Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: `getSeriesCommentsUrl(series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the comments section url for each series\n",
    "# You can select the spectateurs or presse section, or both\n",
    "def getSeriesCommentsUrl(series_df: pd.DataFrame, spect=False, press=False):\n",
    "    \n",
    "    # Get the list of series_id from the series_df\n",
    "    series_id_list = series_df['id'].tolist()\n",
    "\n",
    "    # Set the list\n",
    "    press_url_list = []\n",
    "    user_url_list = []\n",
    "\n",
    "    # Preparing the setting and monitoring of the loop\n",
    "    start_time = time()\n",
    "    p_requests = 1\n",
    "        \n",
    "    for v_id in series_id_list:\n",
    "        if spect:\n",
    "            url_spect = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/'    \n",
    "            user_url_list.append(url_spect)\n",
    "        if press:\n",
    "            url_press = f'https://www.allocine.fr/series/ficheserie-{v_id}/critiques/presse/'   \n",
    "            press_url_list.append(url_press)                         \n",
    "        p_requests += 1\n",
    "\n",
    "    # Saving the files\n",
    "    comments_path = '../Series/Comments/'\n",
    "    print(f'--> Done; {p_requests-1} Series Comments Page Requests in {timedelta(seconds=time()-start_time)}')\n",
    "    if spect:\n",
    "        r = np.asarray(user_url_list)\n",
    "        np.savetxt(f\"{comments_path}user_comments_urls.csv\", r, delimiter=\",\", fmt='%s')\n",
    "    if press:\n",
    "        r = np.asarray(press_url_list)\n",
    "        np.savetxt(f\"{comments_path}press_comments_urls.csv\", r, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `getCommentsUrl(movies_df, series_df, spect, press)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the comments section urls from movies and comments previously retrieved urls.\n",
    "# The comments url list is save as a csv file: movies_comments_url.csv (or series_comments_url.csv)\n",
    "def getCommentsUrl(movies_df=None, series_df=None, spect=False, press=False):\n",
    "    \n",
    "    try:\n",
    "        if movies_df is not None:\n",
    "            getMoviesCommentsUrl(movies_df, spect, press)\n",
    "        if series_df is not None:\n",
    "            getSeriesCommentsUrl(series_df, spect, press)\n",
    "    except:\n",
    "        print('Error in getCommentsUrl function!')\n",
    "        traceback.print_exc()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get press-ratings dataframe: `getPressRatings(urls)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert text to rating: `convertTextToRating(text)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to rating\n",
    "def convert_text_to_rating(text):\n",
    "    if text==\"Nul\":\n",
    "        return 0.5\n",
    "    elif text==\"Très mauvais\":\n",
    "        return 1\n",
    "    elif text==\"Mauvais\":\n",
    "        return 1.5\n",
    "    elif text==\"Pas terrible\":\n",
    "        return 2\n",
    "    elif text==\"Moyen\":\n",
    "        return 2.5  \n",
    "    elif text==\"Pas mal\":\n",
    "        return 3\n",
    "    elif text==\"Bien\":\n",
    "        return 3.5\n",
    "    elif text==\"Très bien\":\n",
    "        return 4\n",
    "    elif text==\"Excellent\":\n",
    "        return 4.5\n",
    "    elif text==\"Chef d'oeuvre\":\n",
    "        return 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main function: `getPressRatings(urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get press_ratings from the comments section url for each movie or series\n",
    "def getPressRatings(press_soup):\n",
    "    press_ratings = {}\n",
    "    div_ratings = press_soup.find_all('li', {'class': 'item'})\n",
    "    if div_ratings:\n",
    "        press_ratings = {id.text.strip():convert_text_to_rating(rating.find('span')['title']) for id, rating in zip(div_ratings, div_ratings)}\n",
    "        return press_ratings\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get user-ratings dataframe: `getUserRatings(urls)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function `ScrapeURL(urls)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the user_ratings from the press and user comments section urls\n",
    "# Returns a datframe and save it as 'movies_press_ratings.csv' (or 'series_press_ratings.csv')\n",
    "def ScrapePressURL(press_url: list, movies=False, series=False):        \n",
    "    # init the dataframe\n",
    "    c = [\"user_id\",\n",
    "        \"id\",\n",
    "        \"user_rating\",\n",
    "    ]\n",
    "    df = pd.DataFrame(columns=c)\n",
    "    \n",
    "    # preparing the setting and monitoring loop\n",
    "    start_time = time()\n",
    "    n_request = 0\n",
    "    \n",
    "    # init list to save errors\n",
    "    errors = []\n",
    "    \n",
    "    # request loop\n",
    "    for url in press_url:\n",
    "        try :\n",
    "            response = urlopen(url)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Monitoring the requests\n",
    "            n_request += 1\n",
    "            \n",
    "            elapsed_time = time() - start_time\n",
    "            print(f'Request #{n_request}; Frequency: {n_request/elapsed_time} requests/s')\n",
    "            clear_output(wait = True)\n",
    "\n",
    "            # Pause the loop\n",
    "            sleep(randint(1,2))\n",
    "\n",
    "            # Warning for non-200 status codes\n",
    "            if response.status != 200:\n",
    "                warn('Request #{}; Status code: {}'.format(n_request, response.status_code))\n",
    "                errors.append(url)\n",
    "\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            html_text = response.read().decode(\"utf-8\")\n",
    "            press_html_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "\n",
    "            # Get the series_id\n",
    "            series_id = url.split('/')[-4].split('-')[-1]            \n",
    "            # Get the press_ratings\n",
    "            press_ratings = getPressRatings(press_html_soup)\n",
    "            for id, rating in press_ratings.items():               \n",
    "                # Append the data\n",
    "                df_tmp = pd.DataFrame({'user_id': [id],\n",
    "                                    'id': [series_id],\n",
    "                                    'user_rating': [rating]\n",
    "                                    })\n",
    "                                    \n",
    "                df = pd.concat([df, df_tmp], ignore_index=True)\n",
    "                \n",
    "        except:\n",
    "            errors.append(url)\n",
    "            warn(f'Request #{n_request} fail; Total errors : {len(errors)}')\n",
    "            traceback.print_exc()\n",
    "            \n",
    "    # monitoring \n",
    "    series_path = '../Series/Ratings/'\n",
    "    elapsed_time = time() - start_time\n",
    "    print(f'Done; {n_request} requests in {timedelta(seconds=elapsed_time)} with {len(errors)} errors')\n",
    "    clear_output(wait = True)\n",
    "    df.to_csv(f\"{series_path}press_ratings_series.csv\", index=False)\n",
    "    # list to dataframe\n",
    "    errors_df = pd.DataFrame(errors, columns=['url'])\n",
    "    errors_df.to_csv(f\"{series_path}press_ratings_errors.csv\")\n",
    "    # return dataframe and errors\n",
    "    return df, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the movies and series dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movies and series dataframes\n",
    "def loadDataFrames():\n",
    "    movies_df = pd.read_csv('../Movies/Data/allocine_movies.csv')\n",
    "    series_df = pd.read_csv('../Series/Data/allocine_series.csv')\n",
    "    return movies_df, series_df\n",
    "movies_df, series_df = loadDataFrames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the comments section urls from spectators and press for movies and series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Done; 15 Series Comments Page Requests in 0:00:00\n"
     ]
    }
   ],
   "source": [
    "getCommentsUrl(movies_df=None, series_df=series_df, spect=True, press=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the comments section urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the press_comments_urls for series\n",
    "press_series_url = pd.read_csv(\"../Series/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_series_url = press_series_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the user_comments_urls for series\n",
    "user_series_url = pd.read_csv(\"../Series/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_series_url = user_series_url['url'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## for movies\\npress_movies_url = pd.read_csv(\"Movies/Comments/press_comments_urls.csv\",names=[\\'url\\'])\\npress_movies_url = press_movies_url[\\'url\\'].tolist()\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the press_comments_urls for movies\n",
    "'''\n",
    "## for movies\n",
    "press_movies_url = pd.read_csv(\"../Movies/Comments/press_comments_urls.csv\",names=['url'])\n",
    "press_movies_url = press_movies_url['url'].tolist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### From the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## for movies\\nuser_movies_url = pd.read_csv(\"Movies/Comments/user_comments_urls.csv\",names=[\\'url\\'])\\nuser_movies_url = user_movies_url[\\'url\\'].tolist()'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the users_comments_urls for movies\n",
    "'''## for movies\n",
    "user_movies_url = pd.read_csv(\"../Movies/Comments/user_comments_urls.csv\",names=['url'])\n",
    "user_movies_url = user_movies_url['url'].tolist()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done; 15 requests in 0:00:57.089078 with 5 errors\n"
     ]
    }
   ],
   "source": [
    "# Scraping the press ratings\n",
    "df_press_ratings, errors_press_ratings = ScrapePressURL(press_series_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>id</th>\n",
       "      <th>user_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20 Minutes</td>\n",
       "      <td>22373</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cosmopolitan</td>\n",
       "      <td>22373</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Critictoo</td>\n",
       "      <td>22373</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ecran Large</td>\n",
       "      <td>26113</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ecran Large</td>\n",
       "      <td>22373</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Variety</td>\n",
       "      <td>26062</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Variety</td>\n",
       "      <td>18529</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Variety</td>\n",
       "      <td>25546</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Washington Post</td>\n",
       "      <td>18529</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>indieWIRE</td>\n",
       "      <td>26062</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id     id  user_rating\n",
       "0        20 Minutes  22373          4.0\n",
       "1      Cosmopolitan  22373          4.0\n",
       "2         Critictoo  22373          4.5\n",
       "3       Ecran Large  26113          3.0\n",
       "4       Ecran Large  22373          1.0\n",
       "..              ...    ...          ...\n",
       "80          Variety  26062          3.5\n",
       "81          Variety  18529          4.0\n",
       "82          Variety  25546          4.0\n",
       "83  Washington Post  18529          3.5\n",
       "84        indieWIRE  26062          4.5\n",
       "\n",
       "[85 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort df_press_ratings by user_id\n",
    "df_press_ratings = df_press_ratings.sort_values(by=['user_id'],ignore_index=True)\n",
    "df_press_ratings"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ff1774c64d83fce3825259fa3771bbe70271854497325f5fa1e2c1b92279703"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
